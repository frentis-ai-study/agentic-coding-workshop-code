# 02: MCP ì±„íŒ… í´ë¼ì´ì–¸íŠ¸

Streamlitê³¼ LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ Part 3 MCP ì„œë²„ì™€ í†µì‹ í•˜ëŠ” ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

## í•™ìŠµ ì‹œê°„

**50ë¶„** (êµ¬ì¡° ì´í•´ 20ë¶„ + ì‹¤ìŠµ 30ë¶„)

## í•™ìŠµ ëª©í‘œ

- âœ… Streamlitìœ¼ë¡œ ì±„íŒ… UI êµ¬ì¶•
- âœ… LangGraph ReAct ì—ì´ì „íŠ¸ êµ¬í˜„
- âœ… langchain-mcp-adaptersë¡œ MCP ì„œë²„ í†µí•©
- âœ… SQLite ëŒ€í™” ê¸°ë¡ ì €ì¥

---

## ê°œìš”

### ë¬´ì—‡ì„ ë§Œë“œë‚˜ìš”?

**ì‚¬ìš©ìê°€ ìì—°ì–´ë¡œ MCP ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì±„íŒ… ì¸í„°í˜ì´ìŠ¤**ë¥¼ ë§Œë“­ë‹ˆë‹¤.

```
ì‚¬ìš©ì: "2 ë”í•˜ê¸° 3ì€?"
  â†“
AI Agent: Calculator ë„êµ¬ í˜¸ì¶œ
  â†“
ì‘ë‹µ: "5ì…ë‹ˆë‹¤"
```

### ì™œ í•„ìš”í•œê°€ìš”?

Part 3ì—ì„œ ë§Œë“  MCP ì„œë²„ëŠ” **ë„êµ¬ ëª¨ìŒì¼ ë¿**, ì¼ë°˜ ì‚¬ìš©ìê°€ ì‚¬ìš©í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.
- âŒ MCP ì„œë²„ ë‹¨ë…: JSON-RPCë¡œë§Œ í†µì‹  ê°€ëŠ¥
- âœ… ì±„íŒ… í´ë¼ì´ì–¸íŠ¸: "ê³„ì‚°í•´ì¤˜" ê°™ì€ ìì—°ì–´ë¡œ ì‚¬ìš©

### í•µì‹¬ êµ¬ì¡°

```mermaid
graph LR
    User[ì‚¬ìš©ì<br/>ìì—°ì–´ ì…ë ¥] --> UI[Streamlit<br/>ì±„íŒ… UI]
    UI --> Agent[LangGraph<br/>ReAct Agent]
    Agent --> MCP[MCP ì„œë²„<br/>Part 3 ë„êµ¬ë“¤]

    Agent --> LLM[Ollama<br/>qwen3-vl:4b]

    MCP --> Tool1[Calculator]
    MCP --> Tool2[Weather]
    MCP --> Tool3[FileSystem]

    UI --> DB[(SQLite<br/>ëŒ€í™” ê¸°ë¡)]

    style User fill:#e1f5ff
    style UI fill:#fff4e1
    style Agent fill:#ccffcc
    style MCP fill:#ffe1f5
    style LLM fill:#ffffcc
```

### ë§Œë“œëŠ” ê²ƒ

| ì»´í¬ë„ŒíŠ¸ | ì—­í•  | ê¸°ìˆ  ìŠ¤íƒ |
|----------|------|----------|
| **Frontend** | ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ | Streamlit |
| **Agent** | ë„êµ¬ ì„ íƒ ë° ì‹¤í–‰ | LangGraph ReAct |
| **MCP Adapter** | MCP í”„ë¡œí† ì½œ í†µì‹  | langchain-mcp-adapters |
| **Database** | ëŒ€í™” ê¸°ë¡ ì €ì¥ | SQLite |
| **LLM** | ìì—°ì–´ ì´í•´ | Ollama (qwen3-vl:4b) |

### ì‹¤í–‰ íë¦„ ì˜ˆì‹œ

```
1. ì‚¬ìš©ì: "ì„œìš¸ ë‚ ì”¨ ì•Œë ¤ì¤˜"
   â†“
2. Streamlit UI: ë©”ì‹œì§€ í‘œì‹œ ë° Agentì— ì „ë‹¬
   â†“
3. LangGraph Agent:
   - LLMì— ì§ˆë¬¸ â†’ "get_weather ë„êµ¬ ì‚¬ìš© í•„ìš”"
   - MCP Adapterë¥¼ í†µí•´ Weather ë„êµ¬ í˜¸ì¶œ
   â†“
4. MCP ì„œë²„: OpenWeatherMap API í˜¸ì¶œ
   â†“
5. Agent: ê²°ê³¼ ë°›ì•„ì„œ ìì—°ì–´ ì‘ë‹µ ìƒì„±
   â†“
6. UI: "ì„œìš¸ì€ í˜„ì¬ 15ë„ì´ê³  ë§‘ìŠµë‹ˆë‹¤" í‘œì‹œ
   â†“
7. SQLite: ëŒ€í™” ë‚´ìš© ì €ì¥
```

---

## ì „ì²´ ì•„í‚¤í…ì²˜

### ì‹œìŠ¤í…œ êµ¬ì„±ë„

```mermaid
graph TB
    subgraph Browser["ë¸Œë¼ìš°ì € (ì‚¬ìš©ì)"]
        User[http://localhost:8501]
    end

    subgraph Streamlit["Streamlit Frontend (app.py)"]
        ChatUI[Chat UI<br/>ë©”ì¸ í˜ì´ì§€]
        HistoryPage[History Page<br/>ëŒ€í™” ê¸°ë¡]
        SettingsPage[Settings Page<br/>ì„¤ì •]
        SessionState[Session State<br/>- messages<br/>- session_id<br/>- agent]

        ChatUI --> SessionState
        HistoryPage --> SessionState
        SettingsPage --> SessionState
    end

    subgraph Backend["Backend Layer (backend/)"]
        Agent[langgraph_agent.py<br/>- MCPAgent<br/>- ReAct íŒ¨í„´<br/>- ë„êµ¬ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜]
        Config[config.py<br/>í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬]
        DB[database.py<br/>- SQLite ì—°ê²°<br/>- ì„¸ì…˜ ê´€ë¦¬<br/>- ë©”ì‹œì§€ CRUD]
        Models[models.py<br/>- ChatMessage<br/>- ChatSession<br/>- MCPServerConfig]
        LLM[OpenAI í˜¸í™˜ API<br/>Ollama/OpenAI<br/>qwen2.5:3b/gpt-4]

        Config --> Agent
        Agent --> LLM
        Agent --> DB
        Models -.-> Agent
        Models -.-> DB
    end

    subgraph MCP["langchain-mcp-adapters"]
        MCPClient[MultiServerMCPClient<br/>- ë³µìˆ˜ MCP ì„œë²„ ê´€ë¦¬<br/>- STDIO/HTTP ì „ì†¡<br/>- ë„êµ¬ ìë™ ë¡œë“œ]
    end

    subgraph MCPServer["Part 3 MCP Server"]
        Calc[Calculator<br/>add/subtract<br/>multiply/divide<br/>power]
        DateTime[DateTime<br/>get_current_datetime<br/>get_timestamp]
        Weather[Weather API<br/>get_weather<br/>get_forecast]
        FileSystem[FileSystem<br/>read_file/write_file<br/>list_files/delete_file]
    end

    subgraph Database["SQLite (chat_history.db)"]
        Sessions[(sessions í…Œì´ë¸”<br/>session_id/title<br/>created_at/updated_at)]
        Messages[(messages í…Œì´ë¸”<br/>id/session_id<br/>role/content<br/>timestamp)]

        Sessions -->|FK| Messages
    end

    User --> ChatUI
    SessionState --> Agent
    Agent -->|Tool Call| MCPClient
    MCPClient -->|STDIO| Calc
    MCPClient -->|STDIO| DateTime
    MCPClient -->|STDIO| Weather
    MCPClient -->|STDIO| FileSystem
    Backend --> Database

    style Browser fill:#e1f5ff
    style Streamlit fill:#fff4e1
    style Backend fill:#f0f0f0
    style MCP fill:#ffe1f5
    style MCPServer fill:#e1ffe1
    style Database fill:#fffacd
```

### ë°ì´í„° íë¦„ (Message Flow)

```mermaid
sequenceDiagram
    actor User as ì‚¬ìš©ì
    participant UI as Streamlit UI
    participant Session as Session State
    participant DB as SQLite DB
    participant Agent as LangGraph Agent
    participant LLM as Ollama/OpenAI

    User->>UI: "5 + 3ì„ ê³„ì‚°í•´ì¤˜"

    Note over UI,Session: 1. ë©”ì‹œì§€ ì„¸ì…˜ì— ì €ì¥
    UI->>Session: messages.append({<br/>role: "user",<br/>content: "5+3 ê³„ì‚°"<br/>})

    Note over Session,DB: 2. ë°ì´í„°ë² ì´ìŠ¤ì— ì˜ì†í™”
    Session->>DB: ChatMessage(<br/>role="user",<br/>session_id="abc-123"<br/>)
    DB-->>Session: ì €ì¥ ì™„ë£Œ

    Note over Agent: 3. ì—ì´ì „íŠ¸ í˜¸ì¶œ
    Session->>Agent: agent.chat(<br/>"5+3 ê³„ì‚°",<br/>history=[...]<br/>)

    Note over Agent,LLM: 4. LLM API í˜¸ì¶œ
    Agent->>LLM: POST /v1/chat/completions<br/>{<br/>model: "qwen2.5:3b",<br/>messages: [...]<br/>}

    Note over LLM: 5. LLM ì‘ë‹µ ìƒì„±
    LLM-->>Agent: {<br/>choices: [{<br/>message: {<br/>content: "5+3=8ì…ë‹ˆë‹¤"<br/>}<br/>}]<br/>}

    Agent-->>Session: response: "5 + 3 = 8ì…ë‹ˆë‹¤"

    Note over Session,DB: 6. ì‘ë‹µ ì €ì¥
    Session->>DB: ChatMessage(<br/>role: "assistant",<br/>content: "5+3=8ì…ë‹ˆë‹¤"<br/>)
    DB-->>Session: ì €ì¥ ì™„ë£Œ

    Note over UI,User: 7. ì‘ë‹µ í‘œì‹œ
    Session->>UI: messages.append({<br/>role: "assistant"<br/>})
    UI->>User: ğŸ’¬ "5 + 3 = 8ì…ë‹ˆë‹¤"
```

### MCP ë„êµ¬ í˜¸ì¶œ íë¦„ (í–¥í›„ í†µí•©)

```mermaid
sequenceDiagram
    actor User as ì‚¬ìš©ì
    participant Agent as LangGraph Agent
    participant Adapter as langchain-mcp-adapters
    participant MCP as Part 3 MCP Server
    participant API as ì™¸ë¶€ API
    participant UI as Streamlit UI

    User->>Agent: ì„œìš¸ ë‚ ì”¨ ì•Œë ¤ì¤˜

    Note over Agent: ë‚ ì”¨ ì •ë³´ í•„ìš”<br/>get_weather ë„êµ¬ ì‚¬ìš©
    Agent->>Adapter: get_weather í˜¸ì¶œ

    Note over Adapter,MCP: MCP í”„ë¡œí† ì½œë¡œ ë³€í™˜
    Adapter->>MCP: STDIO í†µì‹ 

    Note over MCP,API: ë„êµ¬ ì‹¤í–‰
    MCP->>API: ë‚ ì”¨ ìš”ì²­
    API-->>MCP: ë‚ ì”¨ ë°ì´í„°

    Note over MCP: ì˜¨ë„ 15ë„<br/>ë‚ ì”¨ ë§‘ìŒ
    MCP-->>Adapter: ê²°ê³¼ ë°˜í™˜

    Note over Adapter: Tool í˜•ì‹ ë³€í™˜
    Adapter-->>Agent: ToolMessage

    Note over Agent: ì˜¨ë„ 15ë„ ë§‘ìŒ í™•ì¸<br/>ì‚¬ìš©ìì—ê²Œ ì‘ë‹µ ìƒì„±
    Agent->>Agent: ìµœì¢… ë‹µë³€

    Agent-->>UI: ë‚ ì”¨ ì •ë³´ ì „ë‹¬
    UI->>User: ë‚ ì”¨ í‘œì‹œ
```

---

## ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
# Part 4 ì˜ì¡´ì„± ì„¤ì¹˜
cd /path/to/fastmcp-example
uv sync --extra part4-testing

# í™˜ê²½ë³€ìˆ˜ ì„¤ì • (Ollama ì‚¬ìš©)
export OPENAI_API_BASE=http://localhost:11434/v1
export OPENAI_API_KEY=ollama

# ë˜ëŠ” .env íŒŒì¼ ìƒì„±
cp .env.example .env
# .env íŒŒì¼ ìˆ˜ì •
```

### 2. Part 3 MCP ì„œë²„ ì‹œì‘

```bash
# ë³„ë„ í„°ë¯¸ë„ì—ì„œ MCP ì„œë²„ ì‹¤í–‰
cd ../../03-mcp-tools/02-tools
uv run python main.py

# ì„œë²„ê°€ STDIO ëª¨ë“œë¡œ ì‹¤í–‰ë˜ì–´ì•¼ í•¨
```

### 3. ì±„íŒ… ì•± ì‹¤í–‰

```bash
# ì´ ë””ë ‰í† ë¦¬ë¡œ ëŒì•„ì™€ì„œ
cd 04-testing-deployment/02-mcp-chat-client
uv run streamlit run app.py

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8501 ì—´ê¸°
```

---

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
02-mcp-chat-client/
â”œâ”€â”€ README.md                # ì´ íŒŒì¼
â”œâ”€â”€ app.py                   # Streamlit ë©”ì¸ ì•±
â”œâ”€â”€ .env.example             # í™˜ê²½ë³€ìˆ˜ ì˜ˆì‹œ
â”œâ”€â”€ .streamlit/              # Streamlit ì„¤ì •
â”‚   â””â”€â”€ config.toml
â”œâ”€â”€ backend/                 # ë°±ì—”ë“œ ë¡œì§
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py            # ì„¤ì • ê´€ë¦¬
â”‚   â”œâ”€â”€ database.py          # SQLite ì—°ê²°
â”‚   â”œâ”€â”€ mcp_client.py        # MCP í´ë¼ì´ì–¸íŠ¸
â”‚   â”œâ”€â”€ langgraph_agent.py   # LangGraph ì—ì´ì „íŠ¸
â”‚   â””â”€â”€ models.py            # ë°ì´í„° ëª¨ë¸
â”œâ”€â”€ pages/                   # Streamlit í˜ì´ì§€
â”‚   â”œâ”€â”€ 1_Chat.py           # ì±„íŒ… í˜ì´ì§€ (ê¸°ë³¸)
â”‚   â”œâ”€â”€ 2_History.py        # ëŒ€í™” ê¸°ë¡
â”‚   â””â”€â”€ 3_Settings.py       # ì„¤ì •
â”œâ”€â”€ components/              # ì¬ì‚¬ìš© ì»´í¬ë„ŒíŠ¸
â”‚   â””â”€â”€ chat_message.py
â””â”€â”€ tests/                   # í…ŒìŠ¤íŠ¸
    â”œâ”€â”€ test_database.py
    â””â”€â”€ test_mcp_client.py
```

---

## í•µì‹¬ êµ¬í˜„

### 1. Streamlit ì•± (app.py)

ê°„ë‹¨í•œ Echo ë´‡ë¶€í„° ì‹œì‘í•˜ì—¬ ì ì§„ì ìœ¼ë¡œ ê¸°ëŠ¥ ì¶”ê°€:

```python
import streamlit as st

st.title("MCP Chat Client")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ì €ì¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AI ì‘ë‹µ (TODO: LangGraph ì—ì´ì „íŠ¸ ì—°ë™)
    response = f"Echo: {prompt}"

    # ì‘ë‹µ í‘œì‹œ ë° ì €ì¥
    with st.chat_message("assistant"):
        st.markdown(response)
    st.session_state.messages.append({"role": "assistant", "content": response})
```

### 2. LangGraph ì—ì´ì „íŠ¸ (backend/langgraph_agent.py)

```python
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI

def create_agent(tools, model_name="gpt-4o-mini"):
    """LangGraph ReAct ì—ì´ì „íŠ¸ ìƒì„±"""
    llm = ChatOpenAI(model=model_name, temperature=0)
    agent = create_react_agent(llm, tools=tools)
    return agent

# ì‚¬ìš© ì˜ˆì‹œ:
# agent = create_agent(mcp_tools)
# response = agent.invoke({"messages": [{"role": "user", "content": "2+2 ê³„ì‚°í•´ì¤˜"}]})
```

### 3. MCP í´ë¼ì´ì–¸íŠ¸ (backend/mcp_client.py)

```python
from langchain_mcp_adapters import load_mcp_tools
from mcp import ClientSession, StdioServerParameters

async def load_mcp_tools_from_server(server_command, server_args):
    """MCP ì„œë²„ì—ì„œ ë„êµ¬ ë¡œë“œ"""
    server_params = StdioServerParameters(
        command=server_command,
        args=server_args
    )

    async with ClientSession(server_params) as session:
        tools = await load_mcp_tools(session)
        return tools

# ì‚¬ìš© ì˜ˆì‹œ:
# tools = await load_mcp_tools_from_server(
#     "uv",
#     ["run", "python", "../../03-mcp-tools/02-tools/main.py"]
# )
```

---

## ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œ

### Step 1: ê¸°ë³¸ Echo ë´‡ (5ë¶„)

`app.py`ì— ìœ„ì˜ ì½”ë“œë¥¼ ë³µì‚¬í•˜ê³  ì‹¤í–‰:

```bash
uv run streamlit run app.py
```

ë¸Œë¼ìš°ì €ì—ì„œ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ë©´ ê·¸ëŒ€ë¡œ echoë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

---

### Step 2: LangGraph í†µí•© (15ë¶„)

1. **backend/langgraph_agent.py ì‘ì„±** (ìœ„ ì˜ˆì‹œ ì°¸ì¡°)
2. **app.py ì—…ë°ì´íŠ¸**:

```python
from backend.langgraph_agent import create_agent
from langchain_core.messages import HumanMessage

# ì—ì´ì „íŠ¸ ì´ˆê¸°í™” (ì„¸ì…˜ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
if "agent" not in st.session_state:
    # TODO: MCP ë„êµ¬ ë¡œë“œ í›„ ì „ë‹¬
    st.session_state.agent = create_agent(tools=[])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”"):
    # ... (ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ)

    # LangGraph ì—ì´ì „íŠ¸ í˜¸ì¶œ
    result = st.session_state.agent.invoke({
        "messages": st.session_state.messages
    })

    # ì‘ë‹µ ì¶”ì¶œ
    last_message = result["messages"][-1]
    response = last_message.content

    # ... (ì‘ë‹µ í‘œì‹œ)
```

---

### Step 3: MCP ì„œë²„ ì—°ë™ (20ë¶„)

1. **backend/mcp_client.py ì‘ì„±** (ìœ„ ì˜ˆì‹œ ì°¸ì¡°)
2. **app.pyì—ì„œ MCP ë„êµ¬ ë¡œë“œ**:

```python
import asyncio
from backend.mcp_client import load_mcp_tools_from_server

@st.cache_resource
def get_mcp_tools():
    """MCP ì„œë²„ì—ì„œ ë„êµ¬ ë¡œë“œ (ìºì‹±)"""
    tools = asyncio.run(load_mcp_tools_from_server(
        "uv",
        ["run", "python", "../../03-mcp-tools/02-tools/main.py"]
    ))
    return tools

# ì•± ì‹œì‘ ì‹œ ë„êµ¬ ë¡œë“œ
try:
    mcp_tools = get_mcp_tools()
    st.success(f"{len(mcp_tools)}ê°œì˜ MCP ë„êµ¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤")
except Exception as e:
    st.error(f"MCP ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
    mcp_tools = []

# ì—ì´ì „íŠ¸ì— ë„êµ¬ ì „ë‹¬
if "agent" not in st.session_state:
    st.session_state.agent = create_agent(tools=mcp_tools)
```

---

### Step 4: SQLite ëŒ€í™” ê¸°ë¡ (ì„ íƒì‚¬í•­)

**backend/database.py**:

```python
import sqlite3
from pathlib import Path

DB_PATH = Path(__file__).parent.parent / "data" / "chat.db"

def init_db():
    """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    DB_PATH.parent.mkdir(exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS messages (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            session_id TEXT,
            role TEXT,
            content TEXT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    """)

    conn.commit()
    conn.close()

def save_message(session_id, role, content):
    """ë©”ì‹œì§€ ì €ì¥"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    cursor.execute(
        "INSERT INTO messages (session_id, role, content) VALUES (?, ?, ?)",
        (session_id, role, content)
    )

    conn.commit()
    conn.close()
```

**app.pyì—ì„œ ì‚¬ìš©**:

```python
from backend.database import init_db, save_message
import uuid

# ì•± ì‹œì‘ ì‹œ DB ì´ˆê¸°í™”
init_db()

# ì„¸ì…˜ ID ìƒì„±
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())

# ë©”ì‹œì§€ ì €ì¥
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”"):
    save_message(st.session_state.session_id, "user", prompt)
    # ...
    save_message(st.session_state.session_id, "assistant", response)
```

---

## í™˜ê²½ë³€ìˆ˜ ì„¤ì •

**`.env.example`**:

```env
# OpenAI í˜¸í™˜ API ì„¤ì • (Ollama ë˜ëŠ” OpenAI)

# ì˜µì…˜ 1: Ollama (ë¬´ë£Œ)
OPENAI_API_BASE=http://localhost:11434/v1
OPENAI_API_KEY=ollama
# Ollama ëª¨ë¸: qwen2.5:3b, llama3.2:3b ë“±

# ì˜µì…˜ 2: OpenAI (ìœ ë£Œ)
# OPENAI_API_BASE=https://api.openai.com/v1
# OPENAI_API_KEY=sk-proj-...

# MCP ì„œë²„ ê²½ë¡œ
MCP_SERVER_COMMAND=uv
MCP_SERVER_ARGS=run,python,../../03-mcp-tools/02-tools/main.py
```

---

## ì‹¤í–‰ í™•ì¸

### 1. Ollama ì„¤ì • (ë¬´ë£Œ)

```bash
# 1. Ollama ì„¤ì¹˜
curl -fsSL https://ollama.com/install.sh | sh

# 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull qwen2.5:3b

# 3. Ollama ì„œë²„ ì‹¤í–‰ (ìë™ìœ¼ë¡œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ë¨)
ollama serve
```

### 2. MCP ì„œë²„ ì‹œì‘

```bash
cd ../../03-mcp-tools/02-tools
uv run python main.py
```

### 3. ì±„íŒ… ì•± ì‹¤í–‰

```bash
uv run streamlit run app.py
```

### 4. í…ŒìŠ¤íŠ¸

ë¸Œë¼ìš°ì €ì—ì„œ ë‹¤ìŒ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ë³´ì„¸ìš”:

1. "2 + 2ë¥¼ ê³„ì‚°í•´ì¤˜" â†’ ê³„ì‚°ê¸° ë„êµ¬ í˜¸ì¶œ
2. "ì„œìš¸ì˜ ë‚ ì”¨ ì•Œë ¤ì¤˜" â†’ ë‚ ì”¨ ë„êµ¬ í˜¸ì¶œ
3. "README.md íŒŒì¼ ì½ì–´ì¤˜" â†’ íŒŒì¼ì‹œìŠ¤í…œ ë„êµ¬ í˜¸ì¶œ

---

## ì¶”ê°€ ê¸°ëŠ¥ êµ¬í˜„ (TODO)

ì‹œê°„ì´ ìˆë‹¤ë©´ ë‹¤ìŒ ê¸°ëŠ¥ì„ ì¶”ê°€í•´ë³´ì„¸ìš”:

### 1. History í˜ì´ì§€ (`pages/2_History.py`)

```python
import streamlit as st
from backend.database import get_all_messages

st.title("ëŒ€í™” ê¸°ë¡")

messages = get_all_messages()

for msg in messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        st.caption(f"ì‹œê°„: {msg['timestamp']}")
```

### 2. Settings í˜ì´ì§€ (`pages/3_Settings.py`)

```python
import streamlit as st

st.title("ì„¤ì •")

# OpenAI API ì„¤ì •
api_base = st.text_input("API Base URL", value="http://localhost:11434/v1")
api_key = st.text_input("API Key", value="ollama", type="password")

# MCP ì„œë²„ ì„ íƒ
mcp_servers = st.multiselect(
    "ì‚¬ìš©í•  MCP ì„œë²„",
    ["ê³„ì‚°ê¸°", "ë‚ ì”¨", "íŒŒì¼ì‹œìŠ¤í…œ"],
    default=["ê³„ì‚°ê¸°", "ë‚ ì”¨"]
)

if st.button("ì €ì¥"):
    # ì„¤ì • ì €ì¥ ë¡œì§
    st.success("ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
```

### 3. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
# LangGraph ì—ì´ì „íŠ¸ì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í‘œì‹œ
with st.chat_message("assistant"):
    message_placeholder = st.empty()
    full_response = ""

    for chunk in st.session_state.agent.stream({
        "messages": st.session_state.messages
    }):
        full_response += chunk.get("content", "")
        message_placeholder.markdown(full_response + "â–Œ")

    message_placeholder.markdown(full_response)
```

---

## ë¬¸ì œ í•´ê²°

### Q1: `ImportError: cannot import name 'load_mcp_tools'`

```bash
# langchain-mcp-adapters ì„¤ì¹˜ í™•ì¸
uv pip list | grep langchain-mcp-adapters

# ì—†ë‹¤ë©´ ì„¤ì¹˜
uv sync --extra part4-testing
```

### Q2: MCP ì„œë²„ ì—°ê²° ì‹¤íŒ¨

```bash
# 1. MCP ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
ps aux | grep python | grep main.py

# 2. ì„œë²„ ê²½ë¡œ í™•ì¸
ls ../../03-mcp-tools/02-tools/main.py

# 3. ìˆ˜ë™ìœ¼ë¡œ ì„œë²„ ì‹¤í–‰í•˜ì—¬ ì˜¤ë¥˜ í™•ì¸
cd ../../03-mcp-tools/02-tools
uv run python main.py
```

### Q3: Streamlit í¬íŠ¸ ì¶©ëŒ

```bash
# ë‹¤ë¥¸ í¬íŠ¸ë¡œ ì‹¤í–‰
uv run streamlit run app.py --server.port 8502
```

---

## ì°¸ê³  ìë£Œ

- [Streamlit Chat Tutorial](https://docs.streamlit.io/develop/tutorials/llms/conversational-apps)
- [LangGraph ReAct Agent](https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/)
- [langchain-mcp-adapters GitHub](https://github.com/langchain-ai/langchain-mcp-adapters)

---

## ë‹¤ìŒ ë‹¨ê³„

- **[03-docker-deployment](../03-docker-deployment/)**: Dockerë¡œ ë°°í¬í•˜ê¸°
- **[04-ci-cd-pipeline](../04-ci-cd-pipeline/)**: GitHub Actions ìë™í™”

---

**Happy Coding!** ğŸš€
